{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eb47df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_rus.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\maxent_treebank_pos_tagger_tab.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets_json.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\Huawei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c08687f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\huawei\\anaconda3\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from spacy) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from spacy) (2.29.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from spacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from spacy) (67.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from spacy) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.0.2-cp311-cp311-win_amd64.whl (15.9 MB)\n",
      "Requirement already satisfied: colorama in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Erişim engellendi: 'C:\\\\Users\\\\Huawei\\\\anaconda3\\\\Lib\\\\site-packages\\\\~-mpy.libs\\\\libopenblas64__v0.3.23-293-gc2f4bdbb-gcc_10_3_0-2bde3a66a51006b2b53eb373ff767a3f.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87732657",
   "metadata": {},
   "source": [
    "# Sentence Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "560d91d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Susmayın!\n",
      "Saat 16'da Kadıkoy'de buluşalım.\n",
      "Şiddet cahillik \n",
      "göstergesinin en üst sınırıdır Ailede şiddet geleceği karartır.\n",
      "Kadına şiddet, insanlığa ihanettir.\n",
      "Her sessiz kalınan şiddet bir gün sizi \n",
      "bulur.\n",
      "Sevgi insanlığın, şiddet hayvanlığın kanunudur.\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "from nltk import sent_tokenize\n",
    "text = \"\"\"Susmayın! Saat 16'da Kadıkoy'de buluşalım. Şiddet cahillik \n",
    "göstergesinin en üst sınırıdır Ailede şiddet geleceği karartır.\n",
    "Kadına şiddet, insanlığa ihanettir. Her sessiz kalınan şiddet bir gün sizi \n",
    "bulur. Sevgi insanlığın, şiddet hayvanlığın kanunudur.\"\"\"\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ab64ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Susmayın', '!']\n",
      "['Saat', \"16'da\", \"Kadıköy'de\", 'buluşalım', '.']\n",
      "['Şiddet', 'cahillik', 'göstergesinin', 'en', 'üst', 'sınırıdır', '.']\n",
      "['Ailede', 'şiddet', 'geleceği', 'karartır', '.']\n",
      "['Kadına', 'şiddet', ',', 'insanlığa', 'ihanettir', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "sentences = [\n",
    "\"Susmayın!\",\n",
    "\"Saat 16'da Kadıköy'de buluşalım.\",\n",
    "\"Şiddet cahillik göstergesinin en üst sınırıdır.\",\n",
    "\"Ailede şiddet geleceği karartır.\",\n",
    "\"Kadına şiddet, insanlığa ihanettir.\" ]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f456fa10",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b40bd7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'civil'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming and Lemmatizaiton\n",
    "# Porter stemmer / \n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "porter_stemmer = PorterStemmer ()\n",
    "word = 'civilizations'\n",
    "porter_stemmer. stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c462bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "civil\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "# Snowball Stemmer is more advanced and can be used for all languages\n",
    "from nltk.stem. snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='english' )\n",
    "word = 'civilizations'\n",
    "print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b72adc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ekmek'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snowballstemmer import TurkishStemmer\n",
    "turkish_stem = TurkishStemmer()\n",
    "turkish_stem.stemWord('ekmekler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bf62907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'çiçeklik'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snowballstemmer import TurkishStemmer\n",
    "turkish_stem = TurkishStemmer()\n",
    "turkish_stem.stemWord('çiçekliklerim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf12e7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'çiçekli'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snowballstemmer import TurkishStemmer\n",
    "turkish_stem = TurkishStemmer()\n",
    "turkish_stem. stemWord('çiçekliler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71efcb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "civilization\n",
      "and\n",
      "humankind\n",
      "and\n",
      "divider\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm' )\n",
    "word = nlp('civilizations and humankind and dividers')\n",
    "for token in word:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999457c3",
   "metadata": {},
   "source": [
    "# POSTtagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75935806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of Speech:  [('Can', 'MD'), ('you', 'PRP'), ('buy', 'VB'), ('me', 'PRP'), ('a', 'DT'), ('red', 'JJ'), ('chili', 'NN'), ('pepper', 'NN'), ('from', 'IN'), ('grocery', 'NN'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "# POSTtagging\n",
    "import nltk\n",
    "tokens = nltk.word_tokenize(\"Can you buy me a red chili pepper from grocery?\")\n",
    "print(\"Part of Speech: \", nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4268a1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "want VERB\n",
      "an DET\n",
      "early ADJ\n",
      "upgrade NOUN\n"
     ]
    }
   ],
   "source": [
    "# POSTtagging\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I want an early upgrade\")\n",
    "for token in doc:\n",
    "    print(token. text, token. pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40f2f06",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee214f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Legendary/JJ\n",
      "  scientist/NN\n",
      "  (PERSON Albert/NNP Einstein/NNP)\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  in/IN\n",
      "  (GPE Ulm/NNP)\n",
      "  ,/,\n",
      "  (GPE Germany/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ne_chunk\n",
    "\n",
    "sentence = \"Legendary scientist Albert Einstein was born in Ulm, Germany.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "entities = nltk.chunk.ne_chunk(tagged_tokens)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a81d711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Michae Jordan', 'PERSON'), ('Berkeley', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp=spacy.load('en_core_web_sm' )\n",
    "sentence=nlp('Michae Jordan is a professor at Berkeley')\n",
    "\n",
    "print([(X.text,X.label_) for X in sentence.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef64129",
   "metadata": {},
   "source": [
    "# Text Categorization and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a3eb44",
   "metadata": {},
   "source": [
    "1- Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b9d1465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dil işlemede kullanılan kütüphaneler nltk spacy scikitlearn vb\n"
     ]
    }
   ],
   "source": [
    "# Punctuation \n",
    "import string\n",
    "message = 'Dil işlemede kullanılan kütüphaneler: nltk, spacy, scikit-learn vb.'\n",
    "print(message.translate(str.maketrans('', '', string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddc648e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acaba', 've', 'için'}\n"
     ]
    }
   ],
   "source": [
    "# Extracting stop words\n",
    "stop_words = ['acaba', 've', 'bir', 'birçok', 'ama', 'için']\n",
    "message = 'Acaba metindeki dolgu kelimelerini bulmak ve temizlemek için ne yapılmalı?'\n",
    "\n",
    "S1 = set(stop_words)\n",
    "S2 = set(message.lower().split())\n",
    "print(S1.intersection(S2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b985de8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metin içinde geçen kelime sayısı o metin için bir özniteliktir.\n"
     ]
    }
   ],
   "source": [
    "# Lowering the words\n",
    "message = 'Metin içinde geçen kelime sayısı o metin için bir özniteliktir.'\n",
    "print(message.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b6f98b",
   "metadata": {},
   "source": [
    "# -----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "174c0c29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_image</th>\n",
       "      <th>published</th>\n",
       "      <th>site</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72337</th>\n",
       "      <td>\"http://www.diken.com.tr/wp-content/uploads/20...</td>\n",
       "      <td>\"2019-03-04T13:33:00.000+02:00\"</td>\n",
       "      <td>\"diken.com.tr\"</td>\n",
       "      <td>\"Yatırım bankası: Dolar/TL üçüncü çeyrekte 8.9...</td>\n",
       "      <td>\"Yatırım bankası: Dolar/TL üçüncü çeyrekte 8.9...</td>\n",
       "      <td>\"http://www.diken.com.tr/yatirim-bankasi-dolar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72338</th>\n",
       "      <td>\"http://www.diken.com.tr/wp-content/uploads/20...</td>\n",
       "      <td>\"2019-03-04T13:21:00.000+02:00\"</td>\n",
       "      <td>\"diken.com.tr\"</td>\n",
       "      <td>\"PİAR Araştırma: Adana ve Antalya’da ‘millet i...</td>\n",
       "      <td>\"PİAR Araştırma: Adana ve Antalya’da ‘millet i...</td>\n",
       "      <td>\"http://www.diken.com.tr/piar-arastirma-adana-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72339</th>\n",
       "      <td>\"http://www.diken.com.tr/wp-content/uploads/20...</td>\n",
       "      <td>\"2019-03-04T12:20:00.000+02:00\"</td>\n",
       "      <td>\"diken.com.tr\"</td>\n",
       "      <td>\"Renaissance Capital: Merkez Bankası bu hafta ...</td>\n",
       "      <td>\"Renaissance Capital: Merkez Bankası bu hafta ...</td>\n",
       "      <td>\"http://www.diken.com.tr/renaissance-capital-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72340</th>\n",
       "      <td>\"http://www.diken.com.tr/wp-content/uploads/20...</td>\n",
       "      <td>\"2019-03-04T11:53:00.000+02:00\"</td>\n",
       "      <td>\"diken.com.tr\"</td>\n",
       "      <td>\"Ağrı’daki İYİ Partililerin istifa gerekçesi: ...</td>\n",
       "      <td>\"Ağrı’daki İYİ Partililerin istifa gerekçesi: ...</td>\n",
       "      <td>\"http://www.diken.com.tr/agridaki-iyi-partilil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72341</th>\n",
       "      <td>\"http://www.diken.com.tr/wp-content/uploads/20...</td>\n",
       "      <td>\"2019-03-04T11:52:00.000+02:00\"</td>\n",
       "      <td>\"diken.com.tr\"</td>\n",
       "      <td>\"Otomobil pazarı iki ayda yarı yarıya eridi 04...</td>\n",
       "      <td>\"Otomobil pazarı iki ayda yarı yarıya eridi\"</td>\n",
       "      <td>\"http://www.diken.com.tr/otomobil-pazari-iki-a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              main_image  \\\n",
       "id                                                         \n",
       "72337  \"http://www.diken.com.tr/wp-content/uploads/20...   \n",
       "72338  \"http://www.diken.com.tr/wp-content/uploads/20...   \n",
       "72339  \"http://www.diken.com.tr/wp-content/uploads/20...   \n",
       "72340  \"http://www.diken.com.tr/wp-content/uploads/20...   \n",
       "72341  \"http://www.diken.com.tr/wp-content/uploads/20...   \n",
       "\n",
       "                             published            site  \\\n",
       "id                                                       \n",
       "72337  \"2019-03-04T13:33:00.000+02:00\"  \"diken.com.tr\"   \n",
       "72338  \"2019-03-04T13:21:00.000+02:00\"  \"diken.com.tr\"   \n",
       "72339  \"2019-03-04T12:20:00.000+02:00\"  \"diken.com.tr\"   \n",
       "72340  \"2019-03-04T11:53:00.000+02:00\"  \"diken.com.tr\"   \n",
       "72341  \"2019-03-04T11:52:00.000+02:00\"  \"diken.com.tr\"   \n",
       "\n",
       "                                                    text  \\\n",
       "id                                                         \n",
       "72337  \"Yatırım bankası: Dolar/TL üçüncü çeyrekte 8.9...   \n",
       "72338  \"PİAR Araştırma: Adana ve Antalya’da ‘millet i...   \n",
       "72339  \"Renaissance Capital: Merkez Bankası bu hafta ...   \n",
       "72340  \"Ağrı’daki İYİ Partililerin istifa gerekçesi: ...   \n",
       "72341  \"Otomobil pazarı iki ayda yarı yarıya eridi 04...   \n",
       "\n",
       "                                                   title  \\\n",
       "id                                                         \n",
       "72337  \"Yatırım bankası: Dolar/TL üçüncü çeyrekte 8.9...   \n",
       "72338  \"PİAR Araştırma: Adana ve Antalya’da ‘millet i...   \n",
       "72339  \"Renaissance Capital: Merkez Bankası bu hafta ...   \n",
       "72340  \"Ağrı’daki İYİ Partililerin istifa gerekçesi: ...   \n",
       "72341       \"Otomobil pazarı iki ayda yarı yarıya eridi\"   \n",
       "\n",
       "                                                     url  \n",
       "id                                                        \n",
       "72337  \"http://www.diken.com.tr/yatirim-bankasi-dolar...  \n",
       "72338  \"http://www.diken.com.tr/piar-arastirma-adana-...  \n",
       "72339  \"http://www.diken.com.tr/renaissance-capital-m...  \n",
       "72340  \"http://www.diken.com.tr/agridaki-iyi-partilil...  \n",
       "72341  \"http://www.diken.com.tr/otomobil-pazari-iki-a...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"turkish_news_70000.csv\", index_col =\"id\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0643b9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "72337    \"Yatırım bankası: Dolar/TL üçüncü çeyrekte 8.9...\n",
       "72338    \"PİAR Araştırma: Adana ve Antalya’da ‘millet i...\n",
       "72339    \"Renaissance Capital: Merkez Bankası bu hafta ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news = df[\"text\"]\n",
    "df_news.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "32699fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri temizleme işlemi için, bir fonksiyon tanımlayalım ve\n",
    "# Bu fonksiyonu bütün veri üzerinde uygulayalım\n",
    "# Öncesinde naktalawa işaretleri ve storwords kümelerini tanımlayalım\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "punct = string.punctuation\n",
    "# nitk kütüphanesinin etkisiz kelimeler kümesini kullanalım\n",
    "stop_words = stopwords.words('turkish')\n",
    "# stopwords kümesine biz de istediğimiz kelimeleri ekleyebiliriz\n",
    "# Örnek:\n",
    "stop_words.extend([\"bir\", \"kadar\",\"sonra\"])\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    # Metindeki tüm harfleri küçük duruma getirir.\n",
    "    text = text.lower()\n",
    "    # Verisetimizdeki yeni satır karekterleri, boşluk karekteriyle değiştirdik.\n",
    "    text = text.replace(\"\\\\n\",\" \")\n",
    "    # Kesme işareti ve sonrasındaki karekterlerin kaldırılması\n",
    "    text = re.sub(\"'(\\w+)\", \"\", text)\n",
    "    text = re.sub(\"'(\\w+)\",\"\", text)\n",
    "    text = re.sub(r\"\\[|\\]\", ',', text)\n",
    "    # Sayıların Kaldırılması\n",
    "    text = re.sub(\"[0-9]+\", \"\", text) # Optional\n",
    "    # Noktalama işaretlerinin kaldırılması\n",
    "    text = \"\".join(list(map(lambda x:x if x not in punct else \" \", text)))\n",
    "    # Etkisiz kelimelerin bir kismmn kaldırılması\n",
    "    text = \" \".join([i for i in text.split() if i not in stop_words])\n",
    "    # Metinde tek kalan harfleri de çıkartalım\n",
    "    text = \" \".join([i for i in text.split() if len(i) > 1])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e8bd350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Şubat ayında ihracat yüzde 3.7 arttı, ithalat yüzde 18.7 azaldı 04/03/2019 12:20\\\\nTicaret Bakanı Ruhsar Pekcan şubat ayında ihracatın yüzde 3.7 artışla 14 milyar 312 milyon dolar, ithalatın yüzde 18.7 azalışla 16 milyar 161 milyon dolar olarak gerçekleştiğini açıkladı. Fotoğraf: Reuters\\\\nBakan Pekcan şunları söyledi: “ Eskiden kullandığımız Özel Ticaret Sistemine göre de şubat ayında ihracatımız yüzde 3,5 artışla 13 milyar 603 milyon dolar olarak gerçekleşmiştir. ÖTS’ye göre ithalatımız şubat ayında yüzde 16,6 düşüşle 15 milyar 793 milyon dolar seviyesinde gerçekleşmiştir. ” Reklam\"'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0af87b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'şubat ayında ihracat yüzde arttı ithalat yüzde azaldı ticaret bakanı ruhsar pekcan şubat ayında ihracatın yüzde artışla milyar milyon dolar ithalatın yüzde azalışla milyar milyon dolar olarak gerçekleştiğini açıkladı fotoğraf reuters bakan pekcan şunları söyledi eskiden kullandığımız özel ticaret sistemine göre şubat ayında ihracatımız yüzde artışla milyar milyon dolar olarak gerçekleşmiştir öts’ye göre ithalatımız şubat ayında yüzde düşüşle milyar milyon dolar seviyesinde gerçekleşmiştir reklam'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessing(df_news.iloc[5]) # Cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "98dc99ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(500)\n",
    "df = df.drop(columns = [\"main_image\",\"published\",\"title\",\"url\",\"site\"], axis=1)\n",
    "df['cleaned_text'] = df['text'].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4b41e91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72337</th>\n",
       "      <td>\"Yatırım bankası: Dolar/TL üçüncü çeyrekte 8.9...</td>\n",
       "      <td>yatırım bankası dolar tl üçüncü çeyrekte ’ı gö...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72338</th>\n",
       "      <td>\"PİAR Araştırma: Adana ve Antalya’da ‘millet i...</td>\n",
       "      <td>pi̇ar araştırma adana antalya’da ‘millet ittif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72339</th>\n",
       "      <td>\"Renaissance Capital: Merkez Bankası bu hafta ...</td>\n",
       "      <td>renaissance capital merkez bankası hafta baz p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72340</th>\n",
       "      <td>\"Ağrı’daki İYİ Partililerin istifa gerekçesi: ...</td>\n",
       "      <td>ağrı’daki i̇yi̇ partililerin istifa gerekçesi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72341</th>\n",
       "      <td>\"Otomobil pazarı iki ayda yarı yarıya eridi 04...</td>\n",
       "      <td>otomobil pazarı iki ayda yarı yarıya eridi oto...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "id                                                         \n",
       "72337  \"Yatırım bankası: Dolar/TL üçüncü çeyrekte 8.9...   \n",
       "72338  \"PİAR Araştırma: Adana ve Antalya’da ‘millet i...   \n",
       "72339  \"Renaissance Capital: Merkez Bankası bu hafta ...   \n",
       "72340  \"Ağrı’daki İYİ Partililerin istifa gerekçesi: ...   \n",
       "72341  \"Otomobil pazarı iki ayda yarı yarıya eridi 04...   \n",
       "\n",
       "                                            cleaned_text  \n",
       "id                                                        \n",
       "72337  yatırım bankası dolar tl üçüncü çeyrekte ’ı gö...  \n",
       "72338  pi̇ar araştırma adana antalya’da ‘millet ittif...  \n",
       "72339  renaissance capital merkez bankası hafta baz p...  \n",
       "72340  ağrı’daki i̇yi̇ partililerin istifa gerekçesi ...  \n",
       "72341  otomobil pazarı iki ayda yarı yarıya eridi oto...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b4f2c025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72337</th>\n",
       "      <td>\"Yatırım bankası: Dolar/TL üçüncü çeyrekte 8.9...</td>\n",
       "      <td>yatırım bankası dolar tl üçüncü çeyrekte ’ı gö...</td>\n",
       "      <td>[yatırım, bankası, dolar, tl, üçüncü, çeyrekte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72338</th>\n",
       "      <td>\"PİAR Araştırma: Adana ve Antalya’da ‘millet i...</td>\n",
       "      <td>pi̇ar araştırma adana antalya’da ‘millet ittif...</td>\n",
       "      <td>[pi̇ar, araştırma, adana, antalya’da, ‘millet,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72339</th>\n",
       "      <td>\"Renaissance Capital: Merkez Bankası bu hafta ...</td>\n",
       "      <td>renaissance capital merkez bankası hafta baz p...</td>\n",
       "      <td>[renaissance, capital, merkez, bankası, hafta,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72340</th>\n",
       "      <td>\"Ağrı’daki İYİ Partililerin istifa gerekçesi: ...</td>\n",
       "      <td>ağrı’daki i̇yi̇ partililerin istifa gerekçesi ...</td>\n",
       "      <td>[ağrı’daki, i̇yi̇, partililerin, istifa, gerek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72341</th>\n",
       "      <td>\"Otomobil pazarı iki ayda yarı yarıya eridi 04...</td>\n",
       "      <td>otomobil pazarı iki ayda yarı yarıya eridi oto...</td>\n",
       "      <td>[otomobil, pazarı, iki, ayda, yarı, yarıya, er...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "id                                                         \n",
       "72337  \"Yatırım bankası: Dolar/TL üçüncü çeyrekte 8.9...   \n",
       "72338  \"PİAR Araştırma: Adana ve Antalya’da ‘millet i...   \n",
       "72339  \"Renaissance Capital: Merkez Bankası bu hafta ...   \n",
       "72340  \"Ağrı’daki İYİ Partililerin istifa gerekçesi: ...   \n",
       "72341  \"Otomobil pazarı iki ayda yarı yarıya eridi 04...   \n",
       "\n",
       "                                            cleaned_text  \\\n",
       "id                                                         \n",
       "72337  yatırım bankası dolar tl üçüncü çeyrekte ’ı gö...   \n",
       "72338  pi̇ar araştırma adana antalya’da ‘millet ittif...   \n",
       "72339  renaissance capital merkez bankası hafta baz p...   \n",
       "72340  ağrı’daki i̇yi̇ partililerin istifa gerekçesi ...   \n",
       "72341  otomobil pazarı iki ayda yarı yarıya eridi oto...   \n",
       "\n",
       "                                          tokenized_text  \n",
       "id                                                        \n",
       "72337  [yatırım, bankası, dolar, tl, üçüncü, çeyrekte...  \n",
       "72338  [pi̇ar, araştırma, adana, antalya’da, ‘millet,...  \n",
       "72339  [renaissance, capital, merkez, bankası, hafta,...  \n",
       "72340  [ağrı’daki, i̇yi̇, partililerin, istifa, gerek...  \n",
       "72341  [otomobil, pazarı, iki, ayda, yarı, yarıya, er...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tokenized_text\"] = df[\"cleaned_text\"].apply(lambda x: x.split())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76668d63",
   "metadata": {},
   "source": [
    "# LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "954e03ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
      "                                              0.0/2.6 MB ? eta -:--:--\n",
      "                                              0.0/2.6 MB ? eta -:--:--\n",
      "     -                                        0.1/2.6 MB 1.7 MB/s eta 0:00:02\n",
      "     ---                                      0.2/2.6 MB 2.1 MB/s eta 0:00:02\n",
      "     --------                                 0.5/2.6 MB 3.0 MB/s eta 0:00:01\n",
      "     ------------                             0.8/2.6 MB 3.7 MB/s eta 0:00:01\n",
      "     -------------                            0.9/2.6 MB 3.3 MB/s eta 0:00:01\n",
      "     -------------------                      1.2/2.6 MB 3.9 MB/s eta 0:00:01\n",
      "     ------------------------                 1.6/2.6 MB 4.2 MB/s eta 0:00:01\n",
      "     --------------------------               1.8/2.6 MB 4.5 MB/s eta 0:00:01\n",
      "     -------------------------------------    2.5/2.6 MB 5.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.6/2.6 MB 5.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.24.2 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.0.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.10.1)\n",
      "Collecting pandas>=2.0.0 (from pyLDAvis)\n",
      "  Downloading pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "                                              0.0/11.6 MB ? eta -:--:--\n",
      "     -                                        0.4/11.6 MB 8.1 MB/s eta 0:00:02\n",
      "     --                                       0.7/11.6 MB 9.1 MB/s eta 0:00:02\n",
      "     ---                                      1.1/11.6 MB 8.4 MB/s eta 0:00:02\n",
      "     ----                                     1.4/11.6 MB 8.3 MB/s eta 0:00:02\n",
      "     ------                                   1.8/11.6 MB 8.3 MB/s eta 0:00:02\n",
      "     -------                                  2.2/11.6 MB 8.1 MB/s eta 0:00:02\n",
      "     --------                                 2.5/11.6 MB 8.0 MB/s eta 0:00:02\n",
      "     ----------                               2.9/11.6 MB 8.0 MB/s eta 0:00:02\n",
      "     -----------                              3.3/11.6 MB 8.1 MB/s eta 0:00:02\n",
      "     ------------                             3.7/11.6 MB 8.0 MB/s eta 0:00:01\n",
      "     -------------                            4.0/11.6 MB 8.1 MB/s eta 0:00:01\n",
      "     ---------------                          4.4/11.6 MB 8.0 MB/s eta 0:00:01\n",
      "     ----------------                         4.8/11.6 MB 8.1 MB/s eta 0:00:01\n",
      "     ------------------                       5.2/11.6 MB 8.1 MB/s eta 0:00:01\n",
      "     ------------------                       5.4/11.6 MB 8.1 MB/s eta 0:00:01\n",
      "     --------------------                     5.9/11.6 MB 8.2 MB/s eta 0:00:01\n",
      "     ---------------------                    6.2/11.6 MB 7.9 MB/s eta 0:00:01\n",
      "     ----------------------                   6.6/11.6 MB 7.9 MB/s eta 0:00:01\n",
      "     ------------------------                 7.0/11.6 MB 8.0 MB/s eta 0:00:01\n",
      "     ------------------------                 7.1/11.6 MB 7.9 MB/s eta 0:00:01\n",
      "     --------------------------               7.8/11.6 MB 8.0 MB/s eta 0:00:01\n",
      "     ---------------------------              8.1/11.6 MB 8.1 MB/s eta 0:00:01\n",
      "     -----------------------------            8.5/11.6 MB 8.0 MB/s eta 0:00:01\n",
      "     ------------------------------           8.9/11.6 MB 8.0 MB/s eta 0:00:01\n",
      "     --------------------------------         9.4/11.6 MB 8.1 MB/s eta 0:00:01\n",
      "     ---------------------------------        9.8/11.6 MB 8.1 MB/s eta 0:00:01\n",
      "     -----------------------------------      10.3/11.6 MB 8.2 MB/s eta 0:00:01\n",
      "     ------------------------------------     10.6/11.6 MB 8.2 MB/s eta 0:00:01\n",
      "     --------------------------------------   11.1/11.6 MB 8.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  11.5/11.6 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  11.6/11.6 MB 8.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  11.6/11.6 MB 8.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 11.6/11.6 MB 7.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from pyLDAvis) (3.1.2)\n",
      "Requirement already satisfied: numexpr in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.8.4)\n",
      "Collecting funcy (from pyLDAvis)\n",
      "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.5.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from pyLDAvis) (4.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from pyLDAvis) (67.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2022.7)\n",
      "Collecting tzdata>=2022.7 (from pandas>=2.0.0->pyLDAvis)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "                                              0.0/346.6 kB ? eta -:--:--\n",
      "     ------------------------------------  337.9/346.6 kB 10.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 346.6/346.6 kB 5.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.5.0)\n",
      "Collecting numpy>=1.24.2 (from pyLDAvis)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Collecting FuzzyTM>=0.4.0 (from gensim->pyLDAvis)\n",
      "  Downloading FuzzyTM-2.0.9-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from jinja2->pyLDAvis) (2.1.1)\n",
      "Collecting pyfume (from FuzzyTM>=0.4.0->gensim->pyLDAvis)\n",
      "  Downloading pyFUME-0.3.4-py3-none-any.whl (60 kB)\n",
      "                                              0.0/60.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.3/60.3 kB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
      "Collecting numpy>=1.24.2 (from pyLDAvis)\n",
      "  Downloading numpy-1.24.4-cp311-cp311-win_amd64.whl (14.8 MB)\n",
      "                                              0.0/14.8 MB ? eta -:--:--\n",
      "     -                                        0.5/14.8 MB 10.0 MB/s eta 0:00:02\n",
      "     --                                       0.9/14.8 MB 9.8 MB/s eta 0:00:02\n",
      "     ---                                      1.4/14.8 MB 9.7 MB/s eta 0:00:02\n",
      "     ----                                     1.6/14.8 MB 9.5 MB/s eta 0:00:02\n",
      "     -----                                    2.1/14.8 MB 9.0 MB/s eta 0:00:02\n",
      "     ------                                   2.6/14.8 MB 9.1 MB/s eta 0:00:02\n",
      "     --------                                 3.0/14.8 MB 9.1 MB/s eta 0:00:02\n",
      "     ---------                                3.5/14.8 MB 9.2 MB/s eta 0:00:02\n",
      "     ----------                               3.9/14.8 MB 9.1 MB/s eta 0:00:02\n",
      "     -----------                              4.3/14.8 MB 9.2 MB/s eta 0:00:02\n",
      "     ------------                             4.7/14.8 MB 9.5 MB/s eta 0:00:02\n",
      "     -------------                            5.1/14.8 MB 9.3 MB/s eta 0:00:02\n",
      "     --------------                           5.5/14.8 MB 9.3 MB/s eta 0:00:02\n",
      "     ----------------                         5.9/14.8 MB 9.3 MB/s eta 0:00:01\n",
      "     -----------------                        6.4/14.8 MB 9.3 MB/s eta 0:00:01\n",
      "     ------------------                       6.9/14.8 MB 9.3 MB/s eta 0:00:01\n",
      "     -------------------                      7.2/14.8 MB 9.2 MB/s eta 0:00:01\n",
      "     --------------------                     7.6/14.8 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------                    8.1/14.8 MB 9.2 MB/s eta 0:00:01\n",
      "     -----------------------                  8.5/14.8 MB 9.3 MB/s eta 0:00:01\n",
      "     ------------------------                 9.0/14.8 MB 9.2 MB/s eta 0:00:01\n",
      "     -------------------------                9.4/14.8 MB 9.2 MB/s eta 0:00:01\n",
      "     --------------------------               9.8/14.8 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------------              10.3/14.8 MB 9.2 MB/s eta 0:00:01\n",
      "     ----------------------------             10.7/14.8 MB 9.1 MB/s eta 0:00:01\n",
      "     ------------------------------           11.1/14.8 MB 9.2 MB/s eta 0:00:01\n",
      "     -------------------------------          11.6/14.8 MB 9.2 MB/s eta 0:00:01\n",
      "     --------------------------------         12.0/14.8 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------------------        12.4/14.8 MB 9.2 MB/s eta 0:00:01\n",
      "     ----------------------------------       12.9/14.8 MB 9.4 MB/s eta 0:00:01\n",
      "     -----------------------------------      13.3/14.8 MB 9.2 MB/s eta 0:00:01\n",
      "     ------------------------------------     13.7/14.8 MB 9.2 MB/s eta 0:00:01\n",
      "     --------------------------------------   14.2/14.8 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  14.6/14.8 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  14.8/14.8 MB 9.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 14.8/14.8 MB 8.6 MB/s eta 0:00:00\n",
      "Collecting simpful==2.12.0 (from pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis)\n",
      "  Downloading simpful-2.12.0-py3-none-any.whl (24 kB)\n",
      "Collecting fst-pso==1.8.1 (from pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis)\n",
      "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: pip is looking at multiple versions of pyfume to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyfume (from FuzzyTM>=0.4.0->gensim->pyLDAvis)\n",
      "  Downloading pyFUME-0.3.1-py3-none-any.whl (59 kB)\n",
      "                                              0.0/59.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 59.6/59.6 kB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (4.6.3)\n",
      "Collecting miniful (from fst-pso==1.8.1->pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis)\n",
      "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: fst-pso, miniful\n",
      "  Building wheel for fst-pso (setup.py): started\n",
      "  Building wheel for fst-pso (setup.py): finished with status 'done'\n",
      "  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20448 sha256=7170d4ebbc3025f551c328798a292c433a77633f86f7a90a33c862eb95d83cbd\n",
      "  Stored in directory: c:\\users\\huawei\\appdata\\local\\pip\\cache\\wheels\\69\\f5\\e5\\18ad53fe1ed6b2af9fad05ec052e4acbac8e92441df44bad2e\n",
      "  Building wheel for miniful (setup.py): started\n",
      "  Building wheel for miniful (setup.py): finished with status 'done'\n",
      "  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3522 sha256=b1ebf512cbebbbe9f7d2af993f8eb07eb40e4a28281b099b4af93bd70906720b\n",
      "  Stored in directory: c:\\users\\huawei\\appdata\\local\\pip\\cache\\wheels\\9d\\ff\\2f\\afe4cd56f47de147407705626517d68bea0f3b74eb1fb168e6\n",
      "Successfully built fst-pso miniful\n",
      "Installing collected packages: funcy, tzdata, numpy, pandas, simpful, miniful, fst-pso, pyfume, FuzzyTM, pyLDAvis\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.5.3\n",
      "    Uninstalling pandas-1.5.3:\n",
      "      Successfully uninstalled pandas-1.5.3\n",
      "Successfully installed FuzzyTM-2.0.9 fst-pso-1.8.1 funcy-2.0 miniful-0.0.6 numpy-1.26.4 pandas-2.2.3 pyLDAvis-3.4.1 pyfume-0.3.1 simpful-2.12.0 tzdata-2024.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\n",
      "blis 1.0.1 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "numba 0.57.0 requires numpy<1.25,>=1.21, but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "vega 4.0.0 requires pandas<2.0.0,>=1.5.0, but you have pandas 2.2.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "64ffc492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pyLDAvis.gensim \n",
    "# LDA Konu Modellemesi figürsel gösterimi için kullamlan kütüphane\n",
    "\n",
    "# Kelime Listesi - Dictionary Oluşturulması\n",
    "tokenized_text = df[\"tokenized_text\"]\n",
    "word_list = gensim.corpora.Dictionary(tokenized_text)\n",
    "\n",
    "#Kelime Listesi Filtreleme\n",
    "word_list.filter_extremes(no_below=1, no_above=0.7)\n",
    "\n",
    "# Terimlerin Vektörleştirilmesi -- Doküman-Terim Matrisinin Oluşturulması\n",
    "doc_word_matrix = [word_list.doc2bow(word) for word in tokenized_text]\n",
    "\n",
    "# LDA Model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus = doc_word_matrix,\n",
    "id2word = word_list,\n",
    "num_topics = 15,\n",
    "passes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2267f00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.013*\"yüzde\" + 0.006*\"fotoğraf\" + 0.006*\"göre\" + 0.005*\"olarak\" + 0.004*\"türkiye\" + 0.003*\"i̇stanbul\" + 0.003*\"eski\"')\n",
      "(1, '0.004*\"sağlık\" + 0.003*\"göre\" + 0.002*\"var\" + 0.002*\"şehir\" + 0.002*\"değil\" + 0.002*\"yazarın\" + 0.002*\"olduğu\"')\n",
      "(2, '0.003*\"olarak\" + 0.003*\"fotoğraf\" + 0.003*\"büyük\" + 0.003*\"avrupa\" + 0.002*\"tarafından\" + 0.002*\"bin\" + 0.002*\"ifade\"')\n",
      "(3, '0.004*\"olarak\" + 0.003*\"olan\" + 0.003*\"büyük\" + 0.003*\"milyar\" + 0.002*\"kömür\" + 0.002*\"döviz\" + 0.002*\"son\"')\n",
      "(4, '0.004*\"mahkeme\" + 0.004*\"kişi\" + 0.003*\"olan\" + 0.003*\"fotoğraf\" + 0.003*\"karar\" + 0.003*\"türkiye’nin\" + 0.003*\"bina\"')\n",
      "(5, '0.003*\"abd\" + 0.003*\"olarak\" + 0.003*\"yıl\" + 0.002*\"tanzim\" + 0.002*\"karşı\" + 0.002*\"öcalan’ın\" + 0.002*\"iki\"')\n",
      "(6, '0.004*\"yüzde\" + 0.004*\"göre\" + 0.003*\"olarak\" + 0.003*\"genel\" + 0.003*\"türkiye\" + 0.002*\"baz\" + 0.002*\"bin\"')\n",
      "(7, '0.004*\"başkanı\" + 0.004*\"fotoğraf\" + 0.004*\"tez\" + 0.004*\"göre\" + 0.003*\"olarak\" + 0.003*\"genel\" + 0.003*\"adayı\"')\n",
      "(8, '0.004*\"seçim\" + 0.004*\"bin\" + 0.003*\"güven\" + 0.003*\"venezuela\" + 0.003*\"olarak\" + 0.002*\"trump\" + 0.002*\"var\"')\n",
      "(9, '0.010*\"yüzde\" + 0.007*\"yakala\" + 0.006*\"göre\" + 0.004*\"beni̇m\" + 0.004*\"hürriyet\" + 0.003*\"ilk\" + 0.003*\"yer\"')\n",
      "(10, '0.003*\"devrimci\" + 0.003*\"kocamaz\" + 0.003*\"cinayet\" + 0.003*\"i̇yi̇\" + 0.002*\"kayıp\" + 0.002*\"aday\" + 0.002*\"gerçek\"')\n",
      "(11, '0.006*\"milyar\" + 0.004*\"var\" + 0.003*\"fotoğraf\" + 0.003*\"faiz\" + 0.003*\"yıl\" + 0.003*\"milyon\" + 0.003*\"lira\"')\n",
      "(12, '0.005*\"bin\" + 0.005*\"yıl\" + 0.004*\"yüzde\" + 0.004*\"olarak\" + 0.003*\"değil\" + 0.003*\"türkiye\" + 0.003*\"fotoğraf\"')\n",
      "(13, '0.006*\"yüzde\" + 0.006*\"bin\" + 0.005*\"göre\" + 0.004*\"aday\" + 0.004*\"fotoğraf\" + 0.004*\"seçim\" + 0.003*\"olarak\"')\n",
      "(14, '0.004*\"sosyal\" + 0.003*\"adayı\" + 0.003*\"seçim\" + 0.003*\"fotoğraf\" + 0.003*\"yüzde\" + 0.003*\"siber\" + 0.003*\"başkanı\"')\n"
     ]
    }
   ],
   "source": [
    "topics = lda_model.print_topics(num_words = 7)\n",
    "\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06829dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "topic_count_range = range(9, 30, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e2ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_scores_list = list()\n",
    "topic_count_list = list()\n",
    "\n",
    "for topic_count in topic_count_range:\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=doc_word_matrix,\n",
    "                                                id2word=word_list,\n",
    "                                                num_topics=topic_count,\n",
    "                                                passes=10)\n",
    "\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=tokenized_text,\n",
    "                                          dictionary=word_list, coherence='c_v')\n",
    "    \n",
    "    temporary_coherence_score_lda = coherence_model_lda.get_coherence()\n",
    "    \n",
    "    coherence_scores_list.append(temporary_coherence_score_lda)\n",
    "    topic_count_list.append(topic_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7def81cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(topic_count_list, coherence_scores_list, \"-\"),\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Scores\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc9bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda_model.print_topics(num_words = 7)\n",
    "topics = sorted(topics, key = lambda x: x[0])\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc7bea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
