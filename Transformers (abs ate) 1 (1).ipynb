{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6de3d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Huawei\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b0f9d1c74a449286566d3149b3adb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Huawei\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Huawei\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Huawei\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b063c2540764a7c8639f62f3ff11574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919f4fa43ca04b95a6c6ee3284d2c650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3381fadaa94934a09a25924877ec85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598048329353333}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a845632a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b9c9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier([\n",
    "\"I've been waiting for a HuggingFace course my whole life.\",\n",
    "\"I hate this so much!\"\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea1247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "\"This is a course about the Transformers library\",\n",
    "candidate_labels=[\"education\", \"politics\", \"business\"]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c936ded4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Huawei\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4dca2b99b247608de17935c28c920a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Huawei\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Huawei\\.cache\\huggingface\\hub\\models--openai-community--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Huawei\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0c1b7035704c2faed345e24da9f644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"In this course, we will teach you how to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e726926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\"In this course, we will teach you how to\",\n",
    "max_length=30,\n",
    "num_return_sequences=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135e3c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3467e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "question=\"Where do I work?\",\n",
    "context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25601108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sumnarizer = pipeline(\"summarization\")\n",
    "summarizer(\"\"\"\n",
    "America has changed dramaticatly during recent years. Not only has the number of\n",
    "graduates in traditional engineering disciplines such as mechanicat, civil,\n",
    "electrical, chemical, and aeronautical engineering declined, but in most of\n",
    "the premier American universities engineering curricula now concentrate on\n",
    "and encourage largely the study of engineering science. As a resutt, there\n",
    "are declining offerings in engineering subjects dealing with infrastructure,\n",
    "the environment, and related issues, and greater concentration on high\n",
    "techmology subjects, largely supporting increasingly complex scientific\n",
    "developments, While the latter is important, it should not be at the expense\n",
    "of more traditional engineering.\n",
    "\n",
    "Rapidly developing economies such as China and India, as well as other\n",
    "industrial countries in Europe and Asia, contimue to encourage and advance\n",
    "the teaching of engimeering. Both China and India, respectively, graduate\n",
    "six and eight times as many traditional engineers as does the United States.\n",
    "Other industrial coumtries at minimum maintain their output, while America\n",
    "suffers an increasingly serious decline in the number of engineering graduates\n",
    "and a lack of well-educated engineers.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736182fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "translator(\"Ce cours est produit par Hugging Face.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86f2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "· Text classification\n",
    "· Zero-shot classification\n",
    "· Text generation\n",
    ". Text completion (mask filling)\n",
    "· Token classification\n",
    ". Question answering\n",
    "· Summarization\n",
    "· Translation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9418446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "tracker = EmissionsTracker ()\n",
    "tracker . start ()\n",
    "# GPU Intensive code goes here\n",
    "tracker. stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2058b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_inputs = [\n",
    "\"I've been waiting for a HuggingFace course my whole life.\",\n",
    "\"I hate this so much!\"]\n",
    "\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, \n",
    "                   return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "{\n",
    "'input_ids': tensor([\n",
    "[ 101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102],\n",
    "[ 101, 1045, 5223, 2023, 2061, 2172, 999, 102, 0,0,0,0,0,0,0,0]]),\n",
    "'attention_mask': tensor([\n",
    "[1, 1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "[1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0]])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d6b83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)\n",
    "\n",
    "torch.Size([2, 16, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa5701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification. from_pretrained(checkpoint)\n",
    "outputs = model( ** inputs)\n",
    "print(outputs.logits)\n",
    "\n",
    "tensor([[-1.5607, 1.6123],\n",
    "[ 4.1692, -3.3464]], grad_fn =< AddmmBackward>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9440f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn. functional.softmax(outputs.logits, dim =- 1)\n",
    "print(predictions)\n",
    "\n",
    "tensor([[4.0195e-02, 9.5980e-01],\n",
    "[9.9946e-01, 5.4418e-04]], grad_fn =< SoftmaxBackward>)\n",
    "\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16d6f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = TFAutoModel. from_pretrained(checkpoint)\n",
    "outputs = model(inputs)\n",
    "outputs. last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c5cf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = TFAutoModelForSequenceClassification. from_pretrained(checkpoint)\n",
    "outputs = model(inputs)\n",
    "outputs. logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38873b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "predictions = tf.math.softmax(outputs.logits, axis =- 1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d716d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
    "print(type(bert_model))\n",
    "\n",
    "gpt_model = AutoModel. from_pretrained(\"gpt2\")\n",
    "print(type(gpt_model))\n",
    "\n",
    "bart_model = AutoModel.from_pretrained(\"facebook/bart-base\")\n",
    "print(type(bart_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42014db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "bert_config = AutoConfig.from_pretrained(\"bert-base-cased\")\n",
    "print(type(bert_config))\n",
    "\n",
    "gpt_config = AutoConfig.from_pretrained(\"gpt2\")\n",
    "print(type(gpt_config))\n",
    "\n",
    "bart_config = AutoConfig.from_pretrained(\"facebook/bart-base\")\n",
    "print(type(bart_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d1c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BerfConfig\n",
    "\n",
    "bert_config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "print(bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7649ef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same architecture as bert-base-cased\n",
    "\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "bert_config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "bert_model = BertModel(bert_config)\n",
    "\n",
    "# Using only 10 layers instead of 12\n",
    "\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "bert_config = BertConfig.from_pretrained(\"bert-base-cased\", num_hidden_layers=10)\n",
    "bert_model = BertModel(bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c916d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "inputs = tokenizer(\"Let's try to tokenize!\")\n",
    "print(inputs[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb66a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v1\")\n",
    "inputs = tokenizer(\"Let's try to tokenize!\")\n",
    "print(inputs[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aaab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "inputs = tokenizer(\"Let's try to tokenize!\")\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)# =print(inputs[\"input_ids\"])\n",
    "print(tokens)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87536622",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_inputs = tokenizer.prepare_for_model(input_ids)\n",
    "print(final_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a917f2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "sentences = [\n",
    "\"I've been waiting for a HuggingFace course my whole life.\",\n",
    "\"I hate this.\"]\n",
    "\n",
    "tokens = [tokenizer. tokenize(sentence) for sentence in sentences]\n",
    "ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
    "print(ids[0])\n",
    "print(ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596a26d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ids = [[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, \n",
    "        2026, 2878, 2166, 1012], \n",
    "       [1045, 5223, 2023, 1012, 0,0,0,0,0,0,0,0,0,0]]\n",
    "input_ids = torch.tensor(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32f44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "sentences = [\n",
    "\"I've been waiting for a HuggingFace course my whole life.\",\n",
    "\"I hate this.\"]\n",
    "\n",
    "tokens = [tokenizer. tokenize(sentence) for sentence in sentences]\n",
    "ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
    "print(ids[0])\n",
    "print(ids[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb8f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96904e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\",\"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df52b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1300c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a58638",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2938252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"sentence1\"], example[\"sentence2\"], \n",
    "        padding=\"max_length\", truncation=True, max_length=128\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched =True)\n",
    "print(tokenized_datasets.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0248d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"idx\", \"sentence1\", \n",
    "                                                        \"sentence2\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets = tokenized_datasets.with_format(\"torch\")\n",
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8c1253",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].select(range(120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabd9037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637577c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tranformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f432a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_input = [{'question': 'Why is conversion important?',\n",
    "'context': 'The option to convert models between FARM and transformers gives freedom to the user a between frameworks'},\n",
    "{'question': 'How many programming languages does BLOOM support?',\n",
    "'context': \"BLOOM has 176 billion parameter and can generate text in 46 languages natural languages\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20955b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'deepset/roberta-base-squad2'\n",
    "\n",
    "#method 1\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "inputs0 = tokenizer(QA_input[0]['question'], QA_input[0] ['context'], return_tensors=\"pt\")\n",
    "output0 = model( ** inputs0)\n",
    "\n",
    "inputs1 = tokenizer(QA_input[1]['question'], QA_input[1] ['context' ], return_tensors=\"pt\")\n",
    "output1 = model( ** inputs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fe8b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "output0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a85780",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_start_idx = torch.argmax(output1.start_logits)\n",
    "answer_end_idx = torch.argmax(output1.end_logits)\n",
    "\n",
    "answer_tokens = inputs1.input_ids[0, answer_start_idx: answer_end_idx + 1]\n",
    "answer = tokenizer.decode(answer_tokens)\n",
    "print(\"ques: {}\\nanswer: {}\".format(QA_input[1]['question:'],answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee7dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "\n",
    "output_0 = qa(QA_input[0]['question'], QA_input[0]['context'])\n",
    "print(output_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9398daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = qa(QA_input[1]['question'], QA_input[1]['context'])\n",
    "print(output_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5195e967",
   "metadata": {},
   "source": [
    "# Loading Datasets from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe261f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93143c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_datasets(\"dair-ai/emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44da1382",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7941d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting a spesific split\n",
    "data_tr = load_dataset(\"dair-ai/emotion\", split=\"train\")\n",
    "data_tr_sample = load_dataset(\"dair-ai/emotion\", \n",
    "                              split=\"train[:1000]+validation[:200]\")\n",
    "print(data_tr)\n",
    "print(data_tr_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a74bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da321110",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a5fcd9",
   "metadata": {},
   "source": [
    "# Dataset Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b8ed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr.sort(\"en\")\n",
    "data_tr[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bdddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sorted = data_tr.sort(\"label\")\n",
    "data_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83bf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b624ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr.shuffle(seed=42)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d138178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the dataset\n",
    "data_tr.slect([0,10,340, 21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d092094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter\n",
    "data_tr.filter(lambda x: x[\"label\"] ==0)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0717fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting\n",
    "data_tr.train_test_split(test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1039966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b65934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
